{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d9c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The correlation matrix for ride_weather_data shows that as temperatures (Mean_Temp, Mean_Feels_Like, Min_Temp, Max_Temp) increase, the number of trips tends to decrease. While humidity (Mean_Humidity) has only a slight negative impact on trips, cloudiness (Mean_Cloudiness) is positively correlated, suggesting more trips on cloudier days. Wind speed (Mean_Wind_Speed) appears to have minimal influence on ride frequency.\n",
    "\n",
    "# Dependencies and Setup\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530341f3",
   "metadata": {},
   "source": [
    "# Uber Data Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dde16d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files to Load\n",
    "\n",
    "uber_data_paths = [\n",
    "    \"Data/uber_data_aprsep_2014.csv\",\n",
    "    \"Data/uber_data_janjune_2015.csv\"\n",
    "]\n",
    "\n",
    "uber_ride_data_paths = [\n",
    "    \"Data/rides_per_day_aprsep_2014.csv\",\n",
    "    \"Data/rides_per_day_janjune_2015.csv\"\n",
    "]\n",
    "\n",
    "uber_summary_data_paths = [\n",
    "    \"Data/uber_summary_aprsep_2014.csv\",\n",
    "    \"Data/uber_summary_janjune_2015.csv\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101b0c1",
   "metadata": {},
   "source": [
    "#### Uber Data with Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "863eb6e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/uber_data_aprsep_2014.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Read each CSV file and append its DataFrame to the list\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m uber_data_paths:\n\u001b[0;32m----> 6\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     data_frames\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Concatenate all DataFrames in the list into one DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/uber_data_aprsep_2014.csv'"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store uber_ride_data\n",
    "data_frames = []\n",
    "\n",
    "# Read each CSV file and append its DataFrame to the list\n",
    "for file_path in uber_data_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into one DataFrame\n",
    "uber_data = pd.concat(data_frames, ignore_index=True)\n",
    "print(\"Uber Data:\")\n",
    "uber_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc70f9b",
   "metadata": {},
   "source": [
    "#### WIth Bases Listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2021c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store uber_ride_data\n",
    "data_frames = []\n",
    "\n",
    "# Read each CSV file and append its DataFrame to the list\n",
    "for file_path in uber_ride_data_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into one DataFrame\n",
    "uber_ride_data_wb = pd.concat(data_frames, ignore_index=True)\n",
    "uber_ride_data_wb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded9e6e",
   "metadata": {},
   "source": [
    "#### Without Bases Listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_ride_data_c = uber_ride_data_wb.copy()\n",
    "\n",
    "# Remove the \"Base\" column\n",
    "uber_ride_data_c = uber_ride_data_c.drop(columns=['Base'])\n",
    "\n",
    "# Convert the \"Date\" column to datetime if it's not already\n",
    "uber_ride_data_c['Date'] = pd.to_datetime(uber_ride_data_c['Date'])\n",
    "\n",
    "# Group by \"Date\" and calculate the total number of trips for each unique date\n",
    "uber_ride_data = uber_ride_data_c.groupby('Date')['Number of trips'].sum().reset_index()\n",
    "uber_ride_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'Number of trips' data\n",
    "data = uber_ride_data['Number of trips'].values\n",
    "\n",
    "# Perform the Shapiro-Wilk test\n",
    "stat, p = shapiro(data)\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Data looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "    print('Data does not look Gaussian (reject H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a default style\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(uber_ride_data['Number of trips'], bins=50)\n",
    "plt.title('Histogram of Number of Trips')\n",
    "plt.xlabel('Number of trips')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"histogram_of_number_of_trips.svg\", format='svg', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Q-Q Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "stats.probplot(uber_ride_data['Number of trips'], plot=plt)\n",
    "plt.title('Q-Q Plot')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"qq_plot.svg\", format='svg', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46cc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-Cox Transformed Histogram\n",
    "transformed_data, lambda_value = stats.boxcox(uber_ride_data['Number of trips'])\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(transformed_data, bins=50)\n",
    "plt.title('Histogram of Transformed Number of Trips')\n",
    "plt.xlabel('Transformed Number of trips')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"histogram_of_transformed_number_of_trips.svg\", format='svg', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40535587",
   "metadata": {},
   "source": [
    "#### Uber Summary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf18f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store DataFrames\n",
    "data_frames = []\n",
    "\n",
    "# Read each CSV file and append its DataFrame to the list\n",
    "for file_path in uber_summary_data_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into one DataFrame\n",
    "uber_summary_data = pd.concat(data_frames)\n",
    "\n",
    "uber_summary_data = uber_summary_data.reset_index(drop=True)\n",
    "\n",
    "uber_summary_data = uber_summary_data.drop([10, 11, 12])\n",
    "\n",
    "print(\"Uber Summary Data:\")\n",
    "uber_summary_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2fa67",
   "metadata": {},
   "source": [
    "# Weather Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_path = \"Data/weather_data_2014-2015.csv\"\n",
    "weather_daily_path = \"Data/weather_daily_summary_2014-2015.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143395b5",
   "metadata": {},
   "source": [
    "#### Weather Data with Daytimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c90cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each CSV file and append its DataFrame to the list\n",
    "weather_data = pd.read_csv(weather_path)\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f4cd87",
   "metadata": {},
   "source": [
    "#### Weather Data by Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b26bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_daily = pd.read_csv(weather_daily_path)\n",
    "weather_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e3c73",
   "metadata": {},
   "source": [
    "# Weather and Ride Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the merged data\n",
    "ride_weather_data = pd.DataFrame()\n",
    "\n",
    "uber_ride_data['Date'] = pd.to_datetime(uber_ride_data['Date'])\n",
    "weather_daily['Date'] = pd.to_datetime(weather_daily['Date'])\n",
    "\n",
    "\n",
    "# Iterate through unique dates in uber_ride_data\n",
    "for date in uber_ride_data['Date'].unique():\n",
    "    datetime_value = pd.Timestamp(date)\n",
    "    date_part = datetime_value.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Filter rows with the current date in uber_ride_data\n",
    "    uber_rows = uber_ride_data[uber_ride_data['Date'] == date_part]\n",
    "    \n",
    "    # Filter rows with the current date in weather_daily\n",
    "    weather_rows = weather_daily[weather_daily['Date'] == date_part]\n",
    "    \n",
    "    # Create a new DataFrame with the merged data\n",
    "    merged_rows = pd.merge(uber_rows, weather_rows, on='Date')\n",
    "    \n",
    "    # Append the merged data to the overall merged_data DataFrame\n",
    "    ride_weather_data = pd.concat([ride_weather_data, merged_rows], ignore_index=True)\n",
    "\n",
    "#Saves ride_weather_data to a .csv\n",
    "ride_weather_data.to_csv('ride_weather_data.csv', index=False)\n",
    "\n",
    "ride_weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd35df35",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd017f",
   "metadata": {},
   "source": [
    "#### Temp vs. Number of Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp vs. Number of Trips\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(ride_weather_data[\"Number of trips\"], ride_weather_data[\"Mean_Temp\"], marker=\"o\", edgecolors=\"black\", alpha=0.8)\n",
    "plt.title(\"Number of trips vs. Temperature\")\n",
    "plt.xlabel(\"Number of trips\")\n",
    "plt.ylabel(\"Mean Temperature (F)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"number_of_trips_vs_temperature.svg\", format='svg', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94efc1",
   "metadata": {},
   "source": [
    "#### Humidity Vs. Number of Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf95492",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(ride_weather_data[\"Number of trips\"], ride_weather_data[\"Mean_Humidity\"], marker=\"o\", edgecolors=\"black\", alpha=0.8)\n",
    "plt.title(\"Number of trips vs. Humidity\")\n",
    "plt.xlabel(\"Number of trips\")\n",
    "plt.ylabel(\"Mean Humidity (%)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"number_of_trips_vs_humidity.svg\", format='svg', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138af8e",
   "metadata": {},
   "source": [
    "#### Max Temp Vs. Number of Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Temp vs. Number of Trips\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(ride_weather_data[\"Number of trips\"], ride_weather_data[\"Max_Temp\"], marker=\"o\", edgecolors=\"black\", alpha=0.8)\n",
    "plt.title(\"Number of trips vs. Max Temperature\")\n",
    "plt.xlabel(\"Number of trips\")\n",
    "plt.ylabel(\"Max Temperature (F)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"number_of_trips_vs_max_temperature.svg\", format='svg', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e16ead",
   "metadata": {},
   "source": [
    "#### Min Temp Vs. Number of Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12deb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min Temp vs. Number of Trips\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(ride_weather_data[\"Number of trips\"], ride_weather_data[\"Min_Temp\"], marker=\"o\", edgecolors=\"black\", alpha=0.8)\n",
    "plt.title(\"Number of trips vs. Min Temperature\")\n",
    "plt.xlabel(\"Number of trips\")\n",
    "plt.ylabel(\"Min Temperature (F)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"number_of_trips_vs_min_temperature.svg\", format='svg', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985144b9",
   "metadata": {},
   "source": [
    "#### Cloudiness Vs. Number of Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloudiness vs. Number of Trips\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(ride_weather_data[\"Number of trips\"], ride_weather_data[\"Mean_Cloudiness\"], marker=\"o\", edgecolors=\"black\", alpha=0.8)\n",
    "plt.title(\"Number of trips vs. Cloudiness\")\n",
    "plt.xlabel(\"Number of trips\")\n",
    "plt.ylabel(\"Mean Cloudiness (%)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"number_of_trips_vs_cloudiness.svg\", format='svg', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bfeaf5",
   "metadata": {},
   "source": [
    "#### Wind Speed Vs. Number of Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind Speed vs. Number of Trips\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(ride_weather_data[\"Number of trips\"], ride_weather_data[\"Mean_Wind_Speed\"], marker=\"o\", edgecolors=\"black\", alpha=0.8)\n",
    "plt.title(\"Number of trips vs. Wind Speed\")\n",
    "plt.xlabel(\"Number of trips\")\n",
    "plt.ylabel(\"Mean Wind Speed (mph)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"number_of_trips_vs_wind_speed.svg\", format='svg', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796cb2e",
   "metadata": {},
   "source": [
    "# Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e065d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlations\n",
    "correlations = {\n",
    "    \"Mean Humidity\": ride_weather_data[\"Number of trips\"].corr(ride_weather_data[\"Mean_Humidity\"]),\n",
    "    \"Max Temperature\": ride_weather_data[\"Number of trips\"].corr(ride_weather_data[\"Max_Temp\"]),\n",
    "    \"Min Temperature\": ride_weather_data[\"Number of trips\"].corr(ride_weather_data[\"Min_Temp\"]),\n",
    "    \"Mean Cloudiness\": ride_weather_data[\"Number of trips\"].corr(ride_weather_data[\"Mean_Cloudiness\"]),\n",
    "    \"Mean Wind Speed\": ride_weather_data[\"Number of trips\"].corr(ride_weather_data[\"Mean_Wind_Speed\"]),\n",
    "    \"Mean Temperature\": ride_weather_data[\"Number of trips\"].corr(ride_weather_data[\"Mean_Temp\"])\n",
    "}\n",
    "\n",
    "# Convert the correlations to percentages and display them\n",
    "print(\"Correlations (as percentages) between Number of Trips and Weather Metrics:\\n\")\n",
    "for metric, corr in correlations.items():\n",
    "    print(f\"{metric}: {corr*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd6ca84",
   "metadata": {},
   "source": [
    "# Weather and Ride Data  (w/Bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the merged data\n",
    "ride_weather_data_wb = pd.DataFrame()\n",
    "\n",
    "uber_ride_data_wb['Date'] = pd.to_datetime(uber_ride_data_wb['Date'])\n",
    "weather_daily['Date'] = pd.to_datetime(weather_daily['Date'])\n",
    "\n",
    "# Iterate through unique dates in uber_ride_data_wb\n",
    "for date in uber_ride_data_wb['Date'].unique():\n",
    "    date_part = str(date)[:10]  # Convert to string and extract the date part\n",
    "    \n",
    "    # Filter rows with the current date in uber_ride_data_wb\n",
    "    uber_rows = uber_ride_data_wb[uber_ride_data_wb['Date'] == date]\n",
    "    \n",
    "    # Filter rows with the current date in weather_daily\n",
    "    weather_rows = weather_daily[weather_daily['Date'] == date_part]\n",
    "    \n",
    "    # Create a new DataFrame with the merged data\n",
    "    merged_rows = pd.merge(uber_rows, weather_rows, on='Date')\n",
    "    \n",
    "    # Append the merged data to the overall ride_weather_data_wb DataFrame\n",
    "    ride_weather_data_wb = pd.concat([ride_weather_data_wb, merged_rows], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb28e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of weather metrics to compare with Number of Trips\n",
    "weather_metrics = [\"Mean_Temp\", \"Mean_Humidity\", \"Max_Temp\", \"Min_Temp\", \"Mean_Cloudiness\", \"Mean_Wind_Speed\"]\n",
    "\n",
    "for metric in weather_metrics:\n",
    "    # Generate scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(ride_weather_data[\"Number of trips\"], ride_weather_data[metric], marker=\"o\", edgecolors=\"black\", alpha=0.8)\n",
    "    plt.title(f\"Number of trips vs. {metric.replace('_', ' ')}\")\n",
    "    plt.xlabel(\"Number of trips\")\n",
    "    plt.ylabel(metric.replace('_', ' '))\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot as an SVG file\n",
    "    plt.savefig(f\"{metric}_vs_trips.svg\", format=\"svg\")\n",
    "    plt.close()  # Close the current figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022f8fb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ride_weather_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extract relevant columns from the ride_weather_data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m relevant_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of trips\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean_Temp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean_Feels_Like\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin_Temp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMax_Temp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean_Humidity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean_Wind_Speed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean_Cloudiness\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m \u001b[43mride_weather_data\u001b[49m[relevant_columns]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Compute the correlation matrix\u001b[39;00m\n\u001b[1;32m      6\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m filtered_data\u001b[38;5;241m.\u001b[39mcorr()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ride_weather_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract relevant columns from the ride_weather_data\n",
    "relevant_columns = ['Number of trips', 'Mean_Temp', 'Mean_Feels_Like', 'Min_Temp', 'Max_Temp', 'Mean_Humidity', 'Mean_Wind_Speed', 'Mean_Cloudiness']\n",
    "filtered_data = ride_weather_data[relevant_columns]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = filtered_data.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc5a216",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correlation_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plotting the heatmap\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(\u001b[43mcorrelation_matrix\u001b[49m, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelation Matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'correlation_matrix' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Correlation_Matrix.svg\", format='svg', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9111b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
